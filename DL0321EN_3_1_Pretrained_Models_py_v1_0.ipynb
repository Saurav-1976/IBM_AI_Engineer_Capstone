{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77887c9d-c2ff-4dd4-b2c6-64bfa1984219"
   },
   "source": [
    "<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13b30d7e-8769-497a-8ff3-410157a58567"
   },
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "373dcde3-aeb9-47f2-95b4-832192acc8de"
   },
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc030d9b-7972-4688-86b1-205951d1163b"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6773ce55-dd0f-49b4-aa00-0c6d6d4e10a8"
   },
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d762fa9-3236-411c-af87-b8de53f576a8"
   },
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1da2417d-cf35-4426-933e-0f94ab2704b1"
   },
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86e78465-733f-43e0-8404-fe3539697b0e"
   },
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:24:24.324317Z",
     "start_time": "2023-03-10T08:24:24.316570Z"
    },
    "id": "fe98db21-badd-4981-8a56-37f238670c3b"
   },
   "outputs": [],
   "source": [
    "#import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df933b81-9331-4f83-a691-f4000da40400"
   },
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:24:39.852620Z",
     "start_time": "2023-03-10T08:24:25.556450Z"
    },
    "id": "ae3d4906-1ca0-4da4-8e9d-0318c7cf80f3"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feb9d8f2-3a9b-41fe-a4bd-2eb9c0a762f3"
   },
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:24:39.894935Z",
     "start_time": "2023-03-10T08:24:39.889594Z"
    },
    "id": "4ac3c0af-d7d2-4b86-b360-f43231471500"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feef4bee-e4f2-4ecd-9677-1208dce1700b"
   },
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:35:38.929005Z",
     "start_time": "2023-03-10T08:35:38.281410Z"
    },
    "id": "0c1537cb-03a6-4f04-b906-ab9ed76d26ee"
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "from keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cb5437b-c5e6-4967-8de1-c03c746e2ece"
   },
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5819d9d-af9c-429b-914c-64eb76e60ce8"
   },
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d995f70-5dfb-47dd-a116-a11c3235ba8f"
   },
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:41:57.944373Z",
     "start_time": "2023-03-10T08:37:15.028199Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0ae1a20-2062-4f1b-96ce-6e0674975622",
    "outputId": "a8b51481-cfa7-44f7-a47e-8d30661b4bf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-10 09:15:48--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 261482368 (249M) [application/zip]\n",
      "Saving to: ‘concrete_data_week3.zip’\n",
      "\n",
      "concrete_data_week3 100%[===================>] 249.37M  20.7MB/s    in 12s     \n",
      "\n",
      "2023-03-10 09:16:01 (20.7 MB/s) - ‘concrete_data_week3.zip’ saved [261482368/261482368]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T10:56:40.795966Z",
     "start_time": "2023-09-06T10:56:40.784991Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3E8TFCN_zoOB",
    "outputId": "3c5d97cd-1724-40c4-a6b3-60916658312a"
   },
   "outputs": [],
   "source": [
    "!unzip concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1adf0e50-909c-4bdb-89d9-c773a89216ed"
   },
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iInyLaMDz83Z"
   },
   "outputs": [],
   "source": [
    "!rm -r __MACOSX/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d9d9beb-9dd8-4e83-9ca9-9e2c09f19eab"
   },
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "366d2205-be27-4b90-82cc-3dd130fd5fd9"
   },
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4533766-4ae0-432e-b6cd-e32b558fadf0"
   },
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acfc926b-9b40-4ead-81aa-b6f4e3dbfd88"
   },
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:35:44.844664Z",
     "start_time": "2023-03-10T08:35:44.841444Z"
    },
    "id": "eadf578c-16b2-425a-885e-1f5202becc12"
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddea71c8-954b-4a80-9664-dcaa0d589811"
   },
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a4a47fa-9090-4e6e-8c78-d98b03cfc38d"
   },
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "352ef6bc-0d38-4949-808a-a37b0b051871"
   },
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:35:49.357709Z",
     "start_time": "2023-03-10T08:35:49.354374Z"
    },
    "id": "72a935a2-9f94-42e6-b764-e01de7ac1af9"
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:52:47.778892Z",
     "start_time": "2023-03-10T08:52:47.609225Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I2ybowsUzAUO",
    "outputId": "f5435766-bea4-46c5-f7bc-2395a9f44c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concrete_data_week3  concrete_data_week3.zip  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e7df7c6-d10f-466d-bc7b-738eca43d839"
   },
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:54:08.256203Z",
     "start_time": "2023-03-10T08:54:05.721900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14dd3fdb-bc4e-4af5-8c3b-24292f73dcdf",
    "outputId": "f089b959-8138-4a6a-8b8d-d97b794c0829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60d98b33-d357-4a11-ad61-63055ba10334"
   },
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 40,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4537a5e-adcf-4722-95c0-9b277b445bd6"
   },
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:54:33.376382Z",
     "start_time": "2023-03-10T08:54:32.593403Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d4b940f-2dc4-44f0-b26e-a04906e0bcfb",
    "outputId": "c1e5c285-bc96-4e8e-a0c3-6dd9b61f2d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "\n",
    "val_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid/',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83c9dbef-2907-497a-9d4a-4ad2828a563f"
   },
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5afab31-6a54-43a2-ae3f-6a85e9c5ffe8"
   },
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c20de955-f27d-461c-8c71-9b0189014f71"
   },
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ade32f00-7084-47c7-8b46-6350be8350bb"
   },
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:54:40.782988Z",
     "start_time": "2023-03-10T08:54:40.202712Z"
    },
    "id": "e6e60cc3-76f1-4096-ab2f-2ea30f49700c"
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fdc30dd-813a-4b2c-b23d-e48b0f3b5530"
   },
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:54:56.245149Z",
     "start_time": "2023-03-10T08:54:41.451756Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "391d3165-5d15-407d-bb54-9262b92ddffd",
    "outputId": "994e8375-8b45-452d-ddb6-4a9eeed3f098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "382b4886-4d8b-4451-baca-462437fb18e6"
   },
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:54:59.539905Z",
     "start_time": "2023-03-10T08:54:59.499762Z"
    },
    "id": "3d6f3e8a-b880-44cb-96f6-aaa53a23b987"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79deafb4-323b-4cc6-80e8-97a9d8d3703e"
   },
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:55:03.682450Z",
     "start_time": "2023-03-10T08:55:03.656311Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7a33afa-9d85-47e6-8746-5bf8a6378fae",
    "outputId": "ce93ca2d-f886-421f-fad7-8445d4907e9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.functional.Functional at 0x7f0f35a80b50>,\n",
       " <keras.layers.core.dense.Dense at 0x7f0f368c0f10>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "934150ad-a32e-4d67-99cb-e3c8c6f63292"
   },
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "483de117-cda0-4b1e-bf37-18871f0c8977"
   },
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:55:13.267677Z",
     "start_time": "2023-03-10T08:55:13.244394Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93ad07b7-a873-421d-a90f-9f6a8f1644d8",
    "outputId": "0e4c86d5-de1f-417e-e6f9-627c0a256f0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x7f0fd48b52e0>,\n",
       " <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x7f0fd48b5430>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0fd48b5dc0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35ba6ca0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35b6af70>,\n",
       " <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x7f0f35b23580>,\n",
       " <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x7f0f35b2dac0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35b232b0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35ac41c0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35b31670>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35aca2e0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35ad0e80>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35ad0670>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35b3a9d0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35ad9be0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35b3aca0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35ae0c40>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f35adcd30>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35ad9580>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35b2d640>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35b6aee0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0fd48b5f40>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35aea220>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35aec2b0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35aec7c0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35aee580>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35aefb50>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f35aef700>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35af9820>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35afec40>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35afe160>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a83580>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a80610>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35a83ee0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a8fb80>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a87730>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35a8fd30>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f35a878e0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35aa2370>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35ab2070>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35ab23a0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a9bcd0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35abf100>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35abffa0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a42e20>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35af99a0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a4a1c0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35aa69d0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35a4adf0>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f35a4a370>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a57340>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a4d9a0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35a5c370>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a47b20>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a47610>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35a57df0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35aa25b0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35aee2e0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35abf220>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f35a9b070>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a68b50>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a6ba30>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35a6e100>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a6e130>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a73520>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35a731f0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a7fb80>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a077f0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35a0b340>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f35a0b970>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a12a90>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a16490>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35a16370>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a1b910>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a237f0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35a23100>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a29e20>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a2e910>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35a2e280>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f35a2e9d0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a3e670>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f359cb1f0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f359cb7c0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f359d1f40>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f359d72e0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f359c4970>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f359ddfa0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a3ee80>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f359e3400>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35a371c0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f359ddee0>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f359e35b0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a7fe50>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a0bdc0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35a3e5b0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35ba6dc0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35af9190>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35a331f0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a3ac70>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a3aac0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f359eab20>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f359f22b0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f359f25e0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f359f8310>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f359f8a00>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f359fcb80>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f359f2fd0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f359fe910>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f359f8160>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f3598f400>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f3598fe80>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f35988400>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f3599a760>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f3599d670>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f359a9b80>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f359a9eb0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f359b04f0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f359b4250>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f359b4970>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f359bd850>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f359422e0>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f359b0610>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35949a00>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f359532e0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35953400>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35953250>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f3595c490>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35960850>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35960820>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35942580>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35994190>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f3599a580>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f3598fe50>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a3a880>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f359e65e0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35a3abb0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35a83400>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35946040>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f3598f340>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f359681c0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f3596afd0>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f35a2e7c0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f3596afa0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f3595c0d0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35968250>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f359784c0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35973d60>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f3596d250>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35908c40>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f3598f460>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35908f10>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f3596d9d0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35a78250>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f359153a0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35915c70>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f3590eac0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f3591da30>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35927400>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f3591d610>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f3590ec40>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35934a60>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f359345e0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f3591d850>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f358c21c0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f358c2b50>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f35915af0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f358c9790>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f358d2df0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f358d25e0>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f358c2bb0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f369327f0>,\n",
       " <keras.layers.convolutional.conv2d.Conv2D at 0x7f0f36932250>,\n",
       " <keras.layers.normalization.batch_normalization.BatchNormalization at 0x7f0f35b31df0>,\n",
       " <keras.layers.merging.add.Add at 0x7f0f369321c0>,\n",
       " <keras.layers.core.activation.Activation at 0x7f0f35b23fa0>,\n",
       " <keras.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x7f0f35b3abb0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9b50534-db82-48ff-a966-db9c9db20b83"
   },
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:55:22.623283Z",
     "start_time": "2023-03-10T08:55:22.602545Z"
    },
    "id": "e9ab99d5-d322-41e7-9c67-7da3d02c0b81"
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "936f1a04-0bcc-4a29-935e-e20d23cd76cd"
   },
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:55:25.484716Z",
     "start_time": "2023-03-10T08:55:25.423674Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80e16337-42ed-4ee6-ab2c-3fa90719d4ff",
    "outputId": "7bbfa7c2-dc79-4e42-ae1a-4ee109616867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 4098      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31c39eb0-b6c6-4f14-9c26-b861e3d071a0"
   },
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:55:31.941730Z",
     "start_time": "2023-03-10T08:55:31.907012Z"
    },
    "id": "4b9d8738-412b-4b18-afec-ff07dd316960"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74173ec3-81e4-41cb-84f0-047d562b195c"
   },
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:55:57.463183Z",
     "start_time": "2023-03-10T08:55:57.458168Z"
    },
    "id": "d0f49844-0ce6-468a-adc0-17c2d9ddbab5"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(val_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f09403d-0dd1-492b-8630-86fb412f6f5a"
   },
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T09:02:14.580756Z",
     "start_time": "2023-03-10T08:56:12.194448Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b0dc574-8803-44d7-b3d7-57026a9f74c0",
    "outputId": "7e8f038a-efe5-4ab5-92e5-42e990e8f386"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-059fd7758d01>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  fit_history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "301/301 [==============================] - 162s 495ms/step - loss: 0.0246 - accuracy: 0.9921 - val_loss: 0.0066 - val_accuracy: 0.9983\n",
      "Epoch 2/2\n",
      "301/301 [==============================] - 142s 470ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.0046 - val_accuracy: 0.9989\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ddfe7fa-64dc-446e-90f1-4caf95352890"
   },
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acff630c-cf29-45b0-be4a-9fbcfb475b42"
   },
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0424beb7-0c3f-4bb3-af40-14e852aaa92f"
   },
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d02a8457-6201-4efd-9f84-ac65812ba037"
   },
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d42d3248-8125-42c9-9a61-4b142ef12e35"
   },
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
