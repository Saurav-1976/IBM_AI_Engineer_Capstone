{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://cocl.us/pytorch_link_top\">\n",
    "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \">\n",
    "</a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"200\" alt=\"cognitiveclass.ai logo\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Objective</h1><ul><li> How to use linear classifier in pytorch.</li></ul> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Linear  Classifier with PyTorch </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Before you use a  Deep neural network to solve the classification problem,  it 's a good idea to try and solve the problem with the simplest method. You will need the dataset object from the previous section.\n",
    "In this lab, we solve the problem with a linear classifier.\n",
    " You will be asked to determine the maximum accuracy your linear classifier can achieve on the validation data for 5 epochs. We will give some free parameter values if you follow the instructions you will be able to answer the quiz. Just like the other labs there are several steps, but in this lab you will only be quizzed on the final result. </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
    "    <li><a href=\"#download_data\"> Download data</a></li>\n",
    "    <li><a href=\"#data_class\"> Dataset Class</a></li>\n",
    "    <li><a href=\"#trasform_Data_object\">Transform Object and Dataset Object</a></li>\n",
    "    <li><a href=\"#Question\">Question</a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>25 min</strong></p>\n",
    " </div>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the libraries we are going to use for this lab:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T07:53:18.695571Z",
     "iopub.status.busy": "2023-03-13T07:53:18.695334Z",
     "iopub.status.idle": "2023-03-13T07:54:20.971836Z",
     "shell.execute_reply": "2023-03-13T07:54:20.971100Z",
     "shell.execute_reply.started": "2023-03-13T07:53:18.695551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.1-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.7.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision) (4.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.28.2)\n",
      "Collecting torch==1.13.1\n",
      "  Using cached torch-1.13.1-cp38-cp38-manylinux1_x86_64.whl (887.4 MB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.23.5)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchvision) (0.34.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchvision) (67.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (5.12.0)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.39.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (2.8)\n",
      "Installing collected packages: pyparsing, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, kiwisolver, fonttools, cycler, contourpy, nvidia-cudnn-cu11, matplotlib, torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.0+cpu\n",
      "    Uninstalling torch-1.13.0+cpu:\n",
      "      Successfully uninstalled torch-1.13.0+cpu\n",
      "Successfully installed contourpy-1.0.7 cycler-0.11.0 fonttools-4.39.0 kiwisolver-1.4.4 matplotlib-3.7.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pyparsing-3.0.9 torch-1.13.1 torchvision-0.14.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:18:41.070740Z",
     "start_time": "2023-03-10T08:18:37.447985Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-13T07:54:24.953086Z",
     "iopub.status.busy": "2023-03-13T07:54:24.952826Z",
     "iopub.status.idle": "2023-03-13T07:54:26.940124Z",
     "shell.execute_reply": "2023-03-13T07:54:26.939356Z",
     "shell.execute_reply.started": "2023-03-13T07:54:24.953067Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch import optim \n",
    "#import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"download_data\">Download Data</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. <b>skillsnetwork.prepare</b> is a command that's used to download a zip file, unzip it and store it in a specified directory. Locally we store the data in the directory  **/resources/data**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the file that contains the images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-07T09:20:06.374457Z",
     "start_time": "2023-09-07T09:20:06.369968Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-13T07:54:32.384423Z",
     "iopub.status.busy": "2023-03-13T07:54:32.384106Z",
     "iopub.status.idle": "2023-03-13T07:55:00.589952Z",
     "shell.execute_reply": "2023-03-13T07:55:00.588852Z",
     "shell.execute_reply.started": "2023-03-13T07:54:32.384403Z"
    }
   },
   "outputs": [],
   "source": [
    "wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip\n",
    "unzip concrete_crack_images_for_classification.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"data_class\">Dataset Class</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use the previous code to build a dataset class. As before, make sure the even samples are positive, and the odd samples are negative.  In this case, if the parameter <code>train</code> is set to <code>True</code>, use the first 10 000 samples as training data; otherwise, the last 10 000 samples will be used as validation data. Do not forget to sort your files so they are in the same order.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We are using the first 10,000 samples as our training data instead of the available 30,000 to decrease the training time of the model. If you want, you can train it yourself with all 30,000 samples just by modifying 2 lines in the following code chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:21:09.833699Z",
     "start_time": "2023-03-10T08:21:09.788079Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-13T07:56:59.804472Z",
     "iopub.status.busy": "2023-03-13T07:56:59.804218Z",
     "iopub.status.idle": "2023-03-13T07:56:59.820006Z",
     "shell.execute_reply": "2023-03-13T07:56:59.819054Z",
     "shell.execute_reply.started": "2023-03-13T07:56:59.804453Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self,transform=None,train=True):\n",
    "        directory=\"data\"\n",
    "        positive=\"Positive\"\n",
    "        negative=\"Negative\"\n",
    "\n",
    "        positive_file_path=os.path.join(directory,positive)\n",
    "        negative_file_path=os.path.join(directory,negative)\n",
    "        positive_files=[os.path.join(positive_file_path,file) for file in  os.listdir(positive_file_path) if file.endswith(\".jpg\")]\n",
    "        positive_files.sort()\n",
    "        negative_files=[os.path.join(negative_file_path,file) for file in  os.listdir(negative_file_path) if file.endswith(\".jpg\")]\n",
    "        negative_files.sort()\n",
    "        number_of_samples=len(positive_files)+len(negative_files)\n",
    "        self.all_files=[None]*number_of_samples\n",
    "        self.all_files[::2]=positive_files\n",
    "        self.all_files[1::2]=negative_files \n",
    "        # The transform is goint to be used on image\n",
    "        self.transform = transform\n",
    "        #torch.LongTensor\n",
    "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
    "        self.Y[::2]=1\n",
    "        self.Y[1::2]=0\n",
    "        \n",
    "        if train:\n",
    "            self.all_files=self.all_files[0:10000] #Change to 30000 to use the full test dataset\n",
    "            self.Y=self.Y[0:10000] #Change to 30000 to use the full test dataset\n",
    "            self.len=len(self.all_files)\n",
    "        else:\n",
    "            self.all_files=self.all_files[30000:]\n",
    "            self.Y=self.Y[30000:]\n",
    "            self.len=len(self.all_files)    \n",
    "       \n",
    "    # Get the length\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    # Getter\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        image=Image.open(self.all_files[idx])\n",
    "        y=self.Y[idx]\n",
    "          \n",
    "        \n",
    "        # If there is any transform method, apply it onto the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"trasform_Data_object\">Transform Object and Dataset Object</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a transform object, that uses the <code>Compose</code> function. First use the transform <code>ToTensor()</code> and followed by <code>Normalize(mean, std)</code>. The value for <code> mean</code> and <code>std</code> are provided for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:21:11.585800Z",
     "start_time": "2023-03-10T08:21:11.576936Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-13T07:57:05.919983Z",
     "iopub.status.busy": "2023-03-13T07:57:05.919701Z",
     "iopub.status.idle": "2023-03-13T07:57:05.924428Z",
     "shell.execute_reply": "2023-03-13T07:57:05.923712Z",
     "shell.execute_reply.started": "2023-03-13T07:57:05.919961Z"
    }
   },
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "# transforms.ToTensor()\n",
    "#transforms.Normalize(mean, std)\n",
    "#transforms.Compose([])\n",
    "\n",
    "transform =transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean, std)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:21:12.351400Z",
     "start_time": "2023-03-10T08:21:12.212534Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-13T07:57:10.827736Z",
     "iopub.status.busy": "2023-03-13T07:57:10.827465Z",
     "iopub.status.idle": "2023-03-13T07:57:11.026192Z",
     "shell.execute_reply": "2023-03-13T07:57:11.025437Z",
     "shell.execute_reply.started": "2023-03-13T07:57:10.827716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1_linearclassiferPytorch.ipynb\t      images\n",
      "LICENSE.md\t\t\t\t      learning-PyTorch-on-IPU\n",
      "README_first.ipynb\t\t\t      schnet-graph-network\n",
      "concrete_crack_images_for_classification.zip  setup.sh\n",
      "data\t\t\t\t\t      temporal-graph-networks\n",
      "distributed-kge\t\t\t\t      useful-tips\n",
      "finetuning-bert\t\t\t\t      vit-model-training\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create object for the training data  <code>dataset_train</code> and validation <code>dataset_val</code>. Use the transform object to convert the images to tensors using the transform object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:21:14.813574Z",
     "start_time": "2023-03-10T08:21:14.583106Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-13T07:57:24.109233Z",
     "iopub.status.busy": "2023-03-13T07:57:24.108932Z",
     "iopub.status.idle": "2023-03-13T07:57:24.256138Z",
     "shell.execute_reply": "2023-03-13T07:57:24.255430Z",
     "shell.execute_reply.started": "2023-03-13T07:57:24.109211Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_train=Dataset(transform=transform,train=True)\n",
    "dataset_val=Dataset(transform=transform,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  can find the shape of the image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:21:16.804148Z",
     "start_time": "2023-03-10T08:21:16.731791Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-13T07:57:26.310757Z",
     "iopub.status.busy": "2023-03-13T07:57:26.310479Z",
     "iopub.status.idle": "2023-03-13T07:57:26.334946Z",
     "shell.execute_reply": "2023-03-13T07:57:26.334298Z",
     "shell.execute_reply.started": "2023-03-13T07:57:26.310736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 227, 227])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it's a color image with three channels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:21:24.326732Z",
     "start_time": "2023-03-10T08:21:24.310837Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-13T07:57:29.711105Z",
     "iopub.status.busy": "2023-03-13T07:57:29.710755Z",
     "iopub.status.idle": "2023-03-13T07:57:29.715574Z",
     "shell.execute_reply": "2023-03-13T07:57:29.714941Z",
     "shell.execute_reply.started": "2023-03-13T07:57:29.711085Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154587"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_of_image=3*227*227\n",
    "size_of_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Question\"> Question <h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create a custom module for Softmax for two classes,called model. The input size should be the <code>size_of_image</code>, you should record the maximum accuracy achieved on the validation data for the different epochs. For example if the 5 epochs the accuracy was 0.5, 0.2, 0.64,0.77, 0.66 you would select 0.77.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with the following free parameter values:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Parameter Values</b>\n",
    "   <li>learning rate:0.1 </li>\n",
    "   <li>momentum term:0.1 </li>\n",
    "   <li>batch size training:5</li>\n",
    "   <li>Loss function:Cross Entropy Loss </li>\n",
    "   <li>epochs:5</li>\n",
    "   <li>set: torch.manual_seed(0)</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T08:21:53.755721Z",
     "start_time": "2023-03-10T08:21:53.745834Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-13T07:57:32.917779Z",
     "iopub.status.busy": "2023-03-13T07:57:32.917505Z",
     "iopub.status.idle": "2023-03-13T07:57:32.923682Z",
     "shell.execute_reply": "2023-03-13T07:57:32.922837Z",
     "shell.execute_reply.started": "2023-03-13T07:57:32.917758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f28eb4ef990>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Custom Module:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T08:20:20.223812Z",
     "iopub.status.busy": "2023-03-13T08:20:20.223383Z",
     "iopub.status.idle": "2023-03-13T08:20:20.228784Z",
     "shell.execute_reply": "2023-03-13T08:20:20.227920Z",
     "shell.execute_reply.started": "2023-03-13T08:20:20.223789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create logistic_regression class\n",
    "\n",
    "class logistic_regression(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, n_inputs):\n",
    "        super(logistic_regression, self).__init__()\n",
    "        self.linear = nn.Linear(n_inputs, 2)\n",
    "        \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        yhat = torch.sigmoid(self.linear(x))\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model Object:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T08:20:20.846418Z",
     "iopub.status.busy": "2023-03-13T08:20:20.846200Z",
     "iopub.status.idle": "2023-03-13T08:20:20.851772Z",
     "shell.execute_reply": "2023-03-13T08:20:20.851159Z",
     "shell.execute_reply.started": "2023-03-13T08:20:20.846399Z"
    }
   },
   "outputs": [],
   "source": [
    "model = logistic_regression(size_of_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Optimizer:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T08:20:21.392667Z",
     "iopub.status.busy": "2023-03-13T08:20:21.392401Z",
     "iopub.status.idle": "2023-03-13T08:20:21.396588Z",
     "shell.execute_reply": "2023-03-13T08:20:21.396010Z",
     "shell.execute_reply.started": "2023-03-13T08:20:21.392647Z"
    }
   },
   "outputs": [],
   "source": [
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Criterion:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T08:20:21.956355Z",
     "iopub.status.busy": "2023-03-13T08:20:21.956123Z",
     "iopub.status.idle": "2023-03-13T08:20:21.959758Z",
     "shell.execute_reply": "2023-03-13T08:20:21.959200Z",
     "shell.execute_reply.started": "2023-03-13T08:20:21.956335Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Loader Training and Validation:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T08:20:27.586526Z",
     "iopub.status.busy": "2023-03-13T08:20:27.586261Z",
     "iopub.status.idle": "2023-03-13T08:20:27.590815Z",
     "shell.execute_reply": "2023-03-13T08:20:27.589861Z",
     "shell.execute_reply.started": "2023-03-13T08:20:27.586505Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = dataset_train, batch_size = 5)\n",
    "validation_loader = DataLoader(dataset = dataset_val, batch_size = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Train Model with 5 epochs, should take 35 minutes: </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-13T08:20:28.191347Z",
     "iopub.status.busy": "2023-03-13T08:20:28.191074Z",
     "iopub.status.idle": "2023-03-13T08:46:05.355588Z",
     "shell.execute_reply": "2023-03-13T08:46:05.354757Z",
     "shell.execute_reply.started": "2023-03-13T08:20:28.191325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Epoch_loss = 1448.3280029296875\n",
      "Validation Epoch = 0, Accuracy = 0.4952\n",
      "Epoch 1, Epoch_loss = 1547.1043701171875\n",
      "Validation Epoch = 1, Accuracy = 0.4924\n",
      "Epoch 2, Epoch_loss = 1626.071044921875\n",
      "Validation Epoch = 2, Accuracy = 0.4941\n",
      "Epoch 3, Epoch_loss = 1624.0650634765625\n",
      "Validation Epoch = 3, Accuracy = 0.4959\n",
      "Epoch 4, Epoch_loss = 1625.549560546875\n",
      "Validation Epoch = 4, Accuracy = 0.4967\n",
      "Epoch 5, Epoch_loss = 1623.711181640625\n",
      "Validation Epoch = 5, Accuracy = 0.4967\n",
      "Epoch 6, Epoch_loss = 1623.407470703125\n",
      "Validation Epoch = 6, Accuracy = 0.4974\n",
      "Epoch 7, Epoch_loss = 1623.171875\n",
      "Validation Epoch = 7, Accuracy = 0.4975\n",
      "Epoch 8, Epoch_loss = 1623.1502685546875\n",
      "Validation Epoch = 8, Accuracy = 0.4976\n",
      "Epoch 9, Epoch_loss = 1622.8953857421875\n",
      "Validation Epoch = 9, Accuracy = 0.4975\n",
      "Epoch 10, Epoch_loss = 1622.69677734375\n",
      "Validation Epoch = 10, Accuracy = 0.4975\n",
      "Epoch 11, Epoch_loss = 1611.589111328125\n",
      "Validation Epoch = 11, Accuracy = 0.5243\n",
      "Epoch 12, Epoch_loss = 1481.326416015625\n",
      "Validation Epoch = 12, Accuracy = 0.496\n",
      "Epoch 13, Epoch_loss = 1620.3546142578125\n",
      "Validation Epoch = 13, Accuracy = 0.4969\n",
      "Epoch 14, Epoch_loss = 1619.9259033203125\n",
      "Validation Epoch = 14, Accuracy = 0.497\n",
      "Epoch 15, Epoch_loss = 1618.559326171875\n",
      "Validation Epoch = 15, Accuracy = 0.4971\n",
      "Epoch 16, Epoch_loss = 1618.2760009765625\n",
      "Validation Epoch = 16, Accuracy = 0.4972\n",
      "Epoch 17, Epoch_loss = 1412.9915771484375\n",
      "Validation Epoch = 17, Accuracy = 0.7124\n",
      "Epoch 18, Epoch_loss = 1241.64892578125\n",
      "Validation Epoch = 18, Accuracy = 0.6862\n",
      "Epoch 19, Epoch_loss = 1245.572021484375\n",
      "Validation Epoch = 19, Accuracy = 0.4826\n",
      "Epoch 20, Epoch_loss = 1193.337646484375\n",
      "Validation Epoch = 20, Accuracy = 0.7074\n",
      "Epoch 21, Epoch_loss = 1185.010986328125\n",
      "Validation Epoch = 21, Accuracy = 0.7065\n",
      "Epoch 22, Epoch_loss = 1230.5274658203125\n",
      "Validation Epoch = 22, Accuracy = 0.6249\n",
      "Epoch 23, Epoch_loss = 1140.984619140625\n",
      "Validation Epoch = 23, Accuracy = 0.7378\n",
      "Epoch 24, Epoch_loss = 1169.1094970703125\n",
      "Validation Epoch = 24, Accuracy = 0.7416\n",
      "Epoch 25, Epoch_loss = 1132.0721435546875\n",
      "Validation Epoch = 25, Accuracy = 0.7381\n",
      "Epoch 26, Epoch_loss = 1133.789794921875\n",
      "Validation Epoch = 26, Accuracy = 0.6628\n",
      "Epoch 27, Epoch_loss = 1132.2232666015625\n",
      "Validation Epoch = 27, Accuracy = 0.4537\n",
      "Epoch 28, Epoch_loss = 1174.2685546875\n",
      "Validation Epoch = 28, Accuracy = 0.7398\n",
      "Epoch 29, Epoch_loss = 1264.54638671875\n",
      "Validation Epoch = 29, Accuracy = 0.604\n",
      "Epoch 30, Epoch_loss = 1223.74072265625\n",
      "Validation Epoch = 30, Accuracy = 0.6424\n",
      "Epoch 31, Epoch_loss = 1123.4942626953125\n",
      "Validation Epoch = 31, Accuracy = 0.6696\n",
      "Epoch 32, Epoch_loss = 1138.7099609375\n",
      "Validation Epoch = 32, Accuracy = 0.7297\n",
      "Epoch 33, Epoch_loss = 1108.885986328125\n",
      "Validation Epoch = 33, Accuracy = 0.7456\n",
      "Epoch 34, Epoch_loss = 1117.37646484375\n",
      "Validation Epoch = 34, Accuracy = 0.7492\n",
      "Epoch 35, Epoch_loss = 1112.8343505859375\n",
      "Validation Epoch = 35, Accuracy = 0.7515\n",
      "Epoch 36, Epoch_loss = 1399.7103271484375\n",
      "Validation Epoch = 36, Accuracy = 0.4819\n",
      "Epoch 37, Epoch_loss = 1609.2427978515625\n",
      "Validation Epoch = 37, Accuracy = 0.7461\n",
      "Epoch 38, Epoch_loss = 1105.078857421875\n",
      "Validation Epoch = 38, Accuracy = 0.704\n",
      "Epoch 39, Epoch_loss = 1112.298828125\n",
      "Validation Epoch = 39, Accuracy = 0.725\n",
      "Epoch 40, Epoch_loss = 1110.0093994140625\n",
      "Validation Epoch = 40, Accuracy = 0.7211\n",
      "Epoch 41, Epoch_loss = 1101.03369140625\n",
      "Validation Epoch = 41, Accuracy = 0.7343\n",
      "Epoch 42, Epoch_loss = 1110.2974853515625\n",
      "Validation Epoch = 42, Accuracy = 0.7338\n",
      "Epoch 43, Epoch_loss = 1115.2916259765625\n",
      "Validation Epoch = 43, Accuracy = 0.7157\n",
      "Epoch 44, Epoch_loss = 1101.3790283203125\n",
      "Validation Epoch = 44, Accuracy = 0.7571\n",
      "Epoch 45, Epoch_loss = 1100.7596435546875\n",
      "Validation Epoch = 45, Accuracy = 0.7468\n",
      "Epoch 46, Epoch_loss = 1119.8482666015625\n",
      "Validation Epoch = 46, Accuracy = 0.7167\n",
      "Epoch 47, Epoch_loss = 1112.0909423828125\n",
      "Validation Epoch = 47, Accuracy = 0.7537\n",
      "Epoch 48, Epoch_loss = 1103.7003173828125\n",
      "Validation Epoch = 48, Accuracy = 0.7284\n",
      "Epoch 49, Epoch_loss = 1104.4007568359375\n",
      "Validation Epoch = 49, Accuracy = 0.751\n",
      "Epoch 50, Epoch_loss = 1126.400146484375\n",
      "Validation Epoch = 50, Accuracy = 0.7274\n",
      "Epoch 51, Epoch_loss = 1114.1986083984375\n",
      "Validation Epoch = 51, Accuracy = 0.7045\n",
      "Epoch 52, Epoch_loss = 1095.5013427734375\n",
      "Validation Epoch = 52, Accuracy = 0.5024\n",
      "Epoch 53, Epoch_loss = 1103.1334228515625\n",
      "Validation Epoch = 53, Accuracy = 0.765\n",
      "Epoch 54, Epoch_loss = 1104.292724609375\n",
      "Validation Epoch = 54, Accuracy = 0.5863\n",
      "Epoch 55, Epoch_loss = 1089.8157958984375\n",
      "Validation Epoch = 55, Accuracy = 0.7664\n",
      "Epoch 56, Epoch_loss = 1102.0186767578125\n",
      "Validation Epoch = 56, Accuracy = 0.7757\n",
      "Epoch 57, Epoch_loss = 1094.2911376953125\n",
      "Validation Epoch = 57, Accuracy = 0.7216\n",
      "Epoch 58, Epoch_loss = 1107.26220703125\n",
      "Validation Epoch = 58, Accuracy = 0.732\n",
      "Epoch 59, Epoch_loss = 1089.944580078125\n",
      "Validation Epoch = 59, Accuracy = 0.7803\n",
      "Epoch 60, Epoch_loss = 1118.7633056640625\n",
      "Validation Epoch = 60, Accuracy = 0.457\n",
      "Epoch 61, Epoch_loss = 1129.9002685546875\n",
      "Validation Epoch = 61, Accuracy = 0.7795\n",
      "Epoch 62, Epoch_loss = 1145.83203125\n",
      "Validation Epoch = 62, Accuracy = 0.77\n",
      "Epoch 63, Epoch_loss = 1121.5887451171875\n",
      "Validation Epoch = 63, Accuracy = 0.7846\n",
      "Epoch 64, Epoch_loss = 1098.57861328125\n",
      "Validation Epoch = 64, Accuracy = 0.7069\n",
      "Epoch 65, Epoch_loss = 1113.00439453125\n",
      "Validation Epoch = 65, Accuracy = 0.6874\n",
      "Epoch 66, Epoch_loss = 1104.9307861328125\n",
      "Validation Epoch = 66, Accuracy = 0.7898\n",
      "Epoch 67, Epoch_loss = 1126.559326171875\n",
      "Validation Epoch = 67, Accuracy = 0.7862\n",
      "Epoch 68, Epoch_loss = 1094.8809814453125\n",
      "Validation Epoch = 68, Accuracy = 0.7937\n",
      "Epoch 69, Epoch_loss = 1096.2972412109375\n",
      "Validation Epoch = 69, Accuracy = 0.7922\n",
      "Epoch 70, Epoch_loss = 1181.244384765625\n",
      "Validation Epoch = 70, Accuracy = 0.7944\n",
      "Epoch 71, Epoch_loss = 1085.46728515625\n",
      "Validation Epoch = 71, Accuracy = 0.789\n",
      "Epoch 72, Epoch_loss = 1094.08544921875\n",
      "Validation Epoch = 72, Accuracy = 0.7703\n",
      "Epoch 73, Epoch_loss = 1093.731689453125\n",
      "Validation Epoch = 73, Accuracy = 0.7979\n",
      "Epoch 74, Epoch_loss = 1094.518310546875\n",
      "Validation Epoch = 74, Accuracy = 0.7924\n",
      "Epoch 75, Epoch_loss = 1088.1097412109375\n",
      "Validation Epoch = 75, Accuracy = 0.7962\n",
      "Epoch 76, Epoch_loss = 1092.693359375\n",
      "Validation Epoch = 76, Accuracy = 0.7574\n",
      "Epoch 77, Epoch_loss = 1137.9794921875\n",
      "Validation Epoch = 77, Accuracy = 0.7158\n",
      "Epoch 78, Epoch_loss = 1090.546142578125\n",
      "Validation Epoch = 78, Accuracy = 0.7794\n",
      "Epoch 79, Epoch_loss = 1097.055419921875\n",
      "Validation Epoch = 79, Accuracy = 0.788\n",
      "Epoch 80, Epoch_loss = 1087.4722900390625\n",
      "Validation Epoch = 80, Accuracy = 0.7923\n",
      "Epoch 81, Epoch_loss = 1085.9637451171875\n",
      "Validation Epoch = 81, Accuracy = 0.793\n",
      "Epoch 82, Epoch_loss = 1085.7894287109375\n",
      "Validation Epoch = 82, Accuracy = 0.7938\n",
      "Epoch 83, Epoch_loss = 1094.4307861328125\n",
      "Validation Epoch = 83, Accuracy = 0.7688\n",
      "Epoch 84, Epoch_loss = 1091.8131103515625\n",
      "Validation Epoch = 84, Accuracy = 0.703\n",
      "Epoch 85, Epoch_loss = 1101.16552734375\n",
      "Validation Epoch = 85, Accuracy = 0.7926\n",
      "Epoch 86, Epoch_loss = 1090.247802734375\n",
      "Validation Epoch = 86, Accuracy = 0.7675\n",
      "Epoch 87, Epoch_loss = 1094.4757080078125\n",
      "Validation Epoch = 87, Accuracy = 0.7965\n",
      "Epoch 88, Epoch_loss = 1108.73779296875\n",
      "Validation Epoch = 88, Accuracy = 0.7717\n",
      "Epoch 89, Epoch_loss = 1097.56201171875\n",
      "Validation Epoch = 89, Accuracy = 0.7668\n",
      "Epoch 90, Epoch_loss = 1092.22509765625\n",
      "Validation Epoch = 90, Accuracy = 0.7831\n",
      "Epoch 91, Epoch_loss = 1083.1806640625\n",
      "Validation Epoch = 91, Accuracy = 0.7885\n",
      "Epoch 92, Epoch_loss = 1080.834716796875\n",
      "Validation Epoch = 92, Accuracy = 0.8011\n",
      "Epoch 93, Epoch_loss = 1092.2772216796875\n",
      "Validation Epoch = 93, Accuracy = 0.754\n",
      "Epoch 94, Epoch_loss = 1093.0762939453125\n",
      "Validation Epoch = 94, Accuracy = 0.7676\n",
      "Epoch 95, Epoch_loss = 1102.0203857421875\n",
      "Validation Epoch = 95, Accuracy = 0.7964\n",
      "Epoch 96, Epoch_loss = 1090.88525390625\n",
      "Validation Epoch = 96, Accuracy = 0.7896\n",
      "Epoch 97, Epoch_loss = 1092.3035888671875\n",
      "Validation Epoch = 97, Accuracy = 0.7959\n",
      "Epoch 98, Epoch_loss = 1096.484375\n",
      "Validation Epoch = 98, Accuracy = 0.8015\n",
      "Epoch 99, Epoch_loss = 1097.9986572265625\n",
      "Validation Epoch = 99, Accuracy = 0.7889\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs=100\n",
    "loss_list=[]\n",
    "accuracy_list=[]\n",
    "correct=0\n",
    "N_test=len(dataset_val)\n",
    "N_train=len(dataset_train)\n",
    "start_time = time.time()\n",
    "#n_epochs\n",
    "\n",
    "Loss=0\n",
    "start_time = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    b_no=0\n",
    "    loss_t = 0\n",
    "    for x, y in train_loader:\n",
    "\n",
    "        model.train() \n",
    "        b_no+=1\n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #make a prediction \n",
    "        yhat=model(x)\n",
    "        #print(yhat, yhat.shape)\n",
    "\n",
    "        # calculate loss \n",
    "        loss = criterion(yhat, y)\n",
    "\n",
    "        # calculate gradients of parameters \n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        #print(\"Batch {}, Batch_loss = {}\".format(b_no, loss.data))\n",
    "        loss_t+= loss.data\n",
    "        \n",
    "    loss_list.append(loss_t)\n",
    "    print(\"Epoch {}, Epoch_loss = {}\".format(epoch, loss_t))\n",
    "    \n",
    "    correct=0\n",
    "    N_tst=0\n",
    "    for x_test, y_test in validation_loader:\n",
    "        # set model to eval \n",
    "        model.eval()\n",
    "        \n",
    "        #make a prediction \n",
    "        z=model(x_test)\n",
    "        N_tst+=100\n",
    "        #print(yhat, yhat.shape)\n",
    "\n",
    "        #find max \n",
    "        yhat= torch.max(z.data, 1)[1]\n",
    "        #print(yhat, yhat.shape)\n",
    "       \n",
    "        #Calculate misclassified  samples in mini-batch \n",
    "        #hint\n",
    "        correct +=(yhat==y_test).sum().item()\n",
    "        #print(\"Validation batch = {}, Accuracy = {}\".format(N_tst/100, correct/N_tst))\n",
    "   \n",
    "    accuracy=correct/N_test\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(\"Validation Epoch = {}, Accuracy = {}\".format(epoch, accuracy))\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
